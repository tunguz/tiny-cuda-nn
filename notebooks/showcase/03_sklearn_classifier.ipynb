{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d9b13b",
   "metadata": {},
   "source": [
    "# sklearn vs tinycudann_sklearn MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f564b",
   "metadata": {},
   "source": [
    "This notebook compares:\n",
    "- `tinycudann_sklearn.MLPClassifier`\n",
    "- `sklearn.neural_network.MLPClassifier`\n",
    "\n",
    "For each dataset, both models use the same train/test split, standardization, and matching hyperparameters.\n",
    "Reported metrics are fit time and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61830646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /media/tunguz/3139-3535/tiny-cuda-nn\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits, load_iris, make_classification\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier as SklearnMLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tinycudann_sklearn import MLPClassifier as TinyMLPClassifier\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"README.md\").exists() and (p / \"CMakeLists.txt\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"Could not find tiny-cuda-nn repository root.\")\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd().resolve())\n",
    "print(\"Repository root:\", ROOT)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a132dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_model(model_cls, model_name, X_train, y_train, X_test, y_test, common_kwargs):\n",
    "    model = model_cls(**common_kwargs)\n",
    "    t0 = time.perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_seconds = time.perf_counter() - t0\n",
    "    accuracy = float(model.score(X_test, y_test))\n",
    "\n",
    "    result = {\n",
    "        \"model\": model_name,\n",
    "        \"fit_seconds\": float(fit_seconds),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"n_iter\": int(getattr(model, \"n_iter_\", -1)),\n",
    "    }\n",
    "\n",
    "    if model_name == \"tinycudann_sklearn\":\n",
    "        result[\"using_tcnn_backend\"] = bool(getattr(model, \"_using_tcnn\", False))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def benchmark_dataset(name, X, y, common_kwargs):\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.int64)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    tiny_res = run_single_model(\n",
    "        TinyMLPClassifier,\n",
    "        \"tinycudann_sklearn\",\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        common_kwargs,\n",
    "    )\n",
    "\n",
    "    sk_res = run_single_model(\n",
    "        SklearnMLPClassifier,\n",
    "        \"sklearn\",\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        common_kwargs,\n",
    "    )\n",
    "\n",
    "    speedup = sk_res[\"fit_seconds\"] / max(tiny_res[\"fit_seconds\"], 1e-9)\n",
    "\n",
    "    summary = {\n",
    "        \"dataset\": name,\n",
    "        \"n_samples\": int(X.shape[0]),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "        \"n_classes\": int(len(np.unique(y))),\n",
    "        \"tiny\": tiny_res,\n",
    "        \"sklearn\": sk_res,\n",
    "        \"speedup_vs_sklearn\": float(speedup),\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2d1145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: iris\n",
      "  size: samples=150 features=4 classes=3\n",
      "  tinycudann_sklearn: time=0.302s acc=0.9474 iters=57 backend=True\n",
      "  sklearn:            time=0.028s acc=0.9211 iters=18\n",
      "  speedup (sklearn/tiny): 0.09x\n",
      "\n",
      "Dataset: digits\n",
      "  size: samples=1797 features=64 classes=10\n",
      "  tinycudann_sklearn: time=0.245s acc=0.9667 iters=20 backend=True\n",
      "  sklearn:            time=0.071s acc=0.9689 iters=14\n",
      "  speedup (sklearn/tiny): 0.29x\n",
      "\n",
      "Dataset: synthetic_200k\n",
      "  size: samples=200000 features=128 classes=10\n",
      "  tinycudann_sklearn: time=3.236s acc=0.9514 iters=19 backend=False\n",
      "  sklearn:            time=11.317s acc=0.9474 iters=25\n",
      "  speedup (sklearn/tiny): 3.50x\n"
     ]
    }
   ],
   "source": [
    "iris_X, iris_y = load_iris(return_X_y=True)\n",
    "digits_X, digits_y = load_digits(return_X_y=True)\n",
    "synthetic_X, synthetic_y = make_classification(\n",
    "    n_samples=200_000,\n",
    "    n_features=128,\n",
    "    n_informative=64,\n",
    "    n_redundant=8,\n",
    "    n_classes=10,\n",
    "    class_sep=1.5,\n",
    "    flip_y=0.01,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "experiments = [\n",
    "    (\n",
    "        \"iris\",\n",
    "        iris_X,\n",
    "        iris_y,\n",
    "        {\n",
    "            \"hidden_layer_sizes\": (64, 64),\n",
    "            \"max_iter\": 80,\n",
    "            \"batch_size\": 32,\n",
    "            \"learning_rate_init\": 5e-3,\n",
    "            \"random_state\": 42,\n",
    "            \"early_stopping\": True,\n",
    "            \"n_iter_no_change\": 8,\n",
    "            \"validation_fraction\": 0.1,\n",
    "            \"verbose\": False,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"digits\",\n",
    "        digits_X,\n",
    "        digits_y,\n",
    "        {\n",
    "            \"hidden_layer_sizes\": (64, 64),\n",
    "            \"max_iter\": 80,\n",
    "            \"batch_size\": 128,\n",
    "            \"learning_rate_init\": 5e-3,\n",
    "            \"random_state\": 42,\n",
    "            \"early_stopping\": True,\n",
    "            \"n_iter_no_change\": 8,\n",
    "            \"validation_fraction\": 0.1,\n",
    "            \"verbose\": False,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"synthetic_200k\",\n",
    "        synthetic_X,\n",
    "        synthetic_y,\n",
    "        {\n",
    "            \"hidden_layer_sizes\": (128, 64),\n",
    "            \"max_iter\": 30,\n",
    "            \"batch_size\": 1024,\n",
    "            \"learning_rate_init\": 3e-3,\n",
    "            \"random_state\": 42,\n",
    "            \"early_stopping\": True,\n",
    "            \"n_iter_no_change\": 8,\n",
    "            \"validation_fraction\": 0.1,\n",
    "            \"verbose\": False,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, X, y, kwargs in experiments:\n",
    "    results.append(benchmark_dataset(name, X, y, kwargs))\n",
    "\n",
    "for r in results:\n",
    "    print()\n",
    "    print(f\"Dataset: {r['dataset']}\")\n",
    "    print(f\"  size: samples={r['n_samples']} features={r['n_features']} classes={r['n_classes']}\")\n",
    "    print(\n",
    "        \"  tinycudann_sklearn: \"\n",
    "        f\"time={r['tiny']['fit_seconds']:.3f}s \"\n",
    "        f\"acc={r['tiny']['accuracy']:.4f} \"\n",
    "        f\"iters={r['tiny']['n_iter']} \"\n",
    "        f\"backend={r['tiny'].get('using_tcnn_backend')}\"\n",
    "    )\n",
    "    print(\n",
    "        \"  sklearn:            \"\n",
    "        f\"time={r['sklearn']['fit_seconds']:.3f}s \"\n",
    "        f\"acc={r['sklearn']['accuracy']:.4f} \"\n",
    "        f\"iters={r['sklearn']['n_iter']}\"\n",
    "    )\n",
    "    print(f\"  speedup (sklearn/tiny): {r['speedup_vs_sklearn']:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca585921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed.\n"
     ]
    }
   ],
   "source": [
    "# Basic checks to ensure larger-than-Iris datasets are present and results are sane.\n",
    "iris_n = next(r[1].shape[0] for r in experiments if r[0] == \"iris\")\n",
    "for name, X, _, _ in experiments:\n",
    "    if name != \"iris\":\n",
    "        assert X.shape[0] > iris_n, f\"{name} is not larger than iris\"\n",
    "\n",
    "res_by_name = {r[\"dataset\"]: r for r in results}\n",
    "assert res_by_name[\"iris\"][\"tiny\"][\"accuracy\"] >= 0.80\n",
    "assert res_by_name[\"digits\"][\"tiny\"][\"accuracy\"] >= 0.90\n",
    "assert res_by_name[\"synthetic_20k\"][\"tiny\"][\"accuracy\"] >= 0.75\n",
    "\n",
    "print(\"All checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bdeca-8f50-48a4-9c7d-85b477b9db26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
